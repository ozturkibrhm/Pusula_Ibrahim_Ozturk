{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63cc88a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T20:06:21.891743Z",
     "iopub.status.busy": "2025-09-05T20:06:21.891380Z",
     "iopub.status.idle": "2025-09-05T20:06:24.187285Z",
     "shell.execute_reply": "2025-09-05T20:06:24.186097Z"
    },
    "papermill": {
     "duration": 2.304904,
     "end_time": "2025-09-05T20:06:24.189062",
     "exception": false,
     "start_time": "2025-09-05T20:06:21.884158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cleandata/dataset_clean-2.xlsx\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa32b0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T20:06:24.200049Z",
     "iopub.status.busy": "2025-09-05T20:06:24.199519Z",
     "iopub.status.idle": "2025-09-05T20:06:26.658325Z",
     "shell.execute_reply": "2025-09-05T20:06:26.657025Z"
    },
    "papermill": {
     "duration": 2.466369,
     "end_time": "2025-09-05T20:06:26.660510",
     "exception": false,
     "start_time": "2025-09-05T20:06:24.194141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kütüphaneleri import edelim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "# pythonda uyarıları kapatır\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e079ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T20:06:26.671326Z",
     "iopub.status.busy": "2025-09-05T20:06:26.670835Z",
     "iopub.status.idle": "2025-09-05T20:06:35.096416Z",
     "shell.execute_reply": "2025-09-05T20:06:35.095139Z"
    },
    "papermill": {
     "duration": 8.433069,
     "end_time": "2025-09-05T20:06:35.098349",
     "exception": false,
     "start_time": "2025-09-05T20:06:26.665280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidfuzz\r\n",
      "  Downloading rapidfuzz-3.14.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\r\n",
      "Downloading rapidfuzz-3.14.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz\r\n",
      "Successfully installed rapidfuzz-3.14.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rapidfuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3326b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T20:06:35.110356Z",
     "iopub.status.busy": "2025-09-05T20:06:35.110031Z",
     "iopub.status.idle": "2025-09-05T20:06:38.337288Z",
     "shell.execute_reply": "2025-09-05T20:06:38.335680Z"
    },
    "papermill": {
     "duration": 3.235973,
     "end_time": "2025-09-05T20:06:38.339398",
     "exception": false,
     "start_time": "2025-09-05T20:06:35.103425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Kontrol] df kolon sırası korunuyor mu?\n",
      "Aynı mı: True\n",
      "Standardize edilen kolonlar: ['kronik_hastalik', 'alerji', 'tanilar', 'uygulama_yerleri']\n",
      "ASCII çıktı aktif mi?: True\n",
      "\n",
      "✅ Kaydedildi:\n",
      " - Clean_Data_Case_DT_2025_std.xlsx  (orijinal + *_std)\n",
      " - Clean_Data_Case_DT_2025_clean.xlsx    (clean: kolon sırası AYNI, içerik standardize, ASCII)\n"
     ]
    }
   ],
   "source": [
    "# ==================== Text Standardization Pipeline (TR -> ASCII Output) ====================\n",
    "# !pip install rapidfuzz openpyxl\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    from rapidfuzz import process, fuzz\n",
    "except Exception:\n",
    "    raise RuntimeError(\"Lütfen 'rapidfuzz' paketini kurun: pip install rapidfuzz\")\n",
    "\n",
    "# --------- Ayarlar ---------\n",
    "DATA_PATH = \"/kaggle/input/cleandata/dataset_clean-2.xlsx\"  # gerekirse değiştirin\n",
    "SHEET_NAME = 0  # 'Sheet1' olabilir\n",
    "\n",
    "# Çoklu/tek değerli kolonlar\n",
    "COLS_MULTI  = [\"kronik_hastalik\", \"alerji\", \"tanilar\", \"uygulama_yerleri\"]\n",
    "COLS_SINGLE = []  # 'bolum' işlenmiyor\n",
    "\n",
    "# Kanonik kümesine girecek adayların minimum frekansı\n",
    "MIN_COUNT_DEFAULT = 5\n",
    "\n",
    "# Fuzzy eşik: 0-100; ne kadar yüksekse o kadar katı\n",
    "FUZZY_THRESHOLD = 88\n",
    "\n",
    "# (Opsiyonel) manuel eş anlamlı/haritalar: soldaki -> sağdaki kanonik (tümü lower-case olmalı)\n",
    "MANUAL_ALIASES = {\n",
    "    # \"alerji\": {\"penisilin\": \"penisilin alerjisi\"},\n",
    "    # \"tanilar\": {\"herni\": \"disk hernisi\"},\n",
    "}\n",
    "\n",
    "# Çıktı dosya adları\n",
    "OUT_STD_ALL = \"Clean_Data_Case_DT_2025_std.xlsx\"    # orijinal + *_std sütunları\n",
    "OUT_CLEAN   = \"Clean_Data_Case_DT_2025_clean.xlsx\"  # kolon sırası AYNI, içerik temiz\n",
    "\n",
    "# Çıktıyı ASCII (İngilizce karakter) ister misin?\n",
    "OUTPUT_ASCII = True\n",
    "\n",
    "# --------- Yardımcılar ---------\n",
    "TR_KEEP = \"a-zA-Z0-9çğıöşüÇĞİÖŞÜıİ\"\n",
    "NON_WORDS = re.compile(fr\"[^\\s{TR_KEEP}]+\")\n",
    "\n",
    "def std_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def tr_lower(s):\n",
    "    \"\"\"Türkçe'ye duyarlı küçük harfe çevirme: İ->i, I->ı, sonra .lower()\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s = str(s)\n",
    "    s = s.replace(\"İ\", \"i\").replace(\"I\", \"ı\")\n",
    "    return s.lower()\n",
    "\n",
    "def normalize_basic(s: str) -> str:\n",
    "    \"\"\"Boşluk, noktalama sadeleştirme, TR karakterlerini koruyarak basit normalize.\"\"\"\n",
    "    if pd.isna(s): \n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = tr_lower(s)\n",
    "    s = s.replace(\"-\", \" \").replace(\"–\", \" \").replace(\"—\", \" \")\n",
    "    s = NON_WORDS.sub(\" \", s)     # TR harfleri hariç noktalama temizliği\n",
    "    s = std_space(s)\n",
    "    return s\n",
    "\n",
    "def to_ascii(text: str) -> str:\n",
    "    \"\"\"Türkçe karakterleri ASCII'ye çevir + diakritik temizliği\"\"\"\n",
    "    if text is None or (isinstance(text, float) and np.isnan(text)):\n",
    "        return text\n",
    "    t = str(text)\n",
    "    # Önce genel diakritik temizliği (örn: â -> a)\n",
    "    t_norm = unicodedata.normalize(\"NFD\", t)\n",
    "    t_no_diac = \"\".join(ch for ch in t_norm if unicodedata.category(ch) != \"Mn\")\n",
    "    # TR özgü harfler (lower case bekliyoruz ama yine de tüm varyantları kapsayalım)\n",
    "    tr_map = {\n",
    "        \"ç\": \"c\", \"Ç\": \"C\",\n",
    "        \"ğ\": \"g\", \"Ğ\": \"G\",\n",
    "        \"ı\": \"i\", \"I\": \"I\",  # I olduğu gibi kalabilir; metin zaten lower'da i/ı'ya dönüyor\n",
    "        \"ö\": \"o\", \"Ö\": \"O\",\n",
    "        \"ş\": \"s\", \"Ş\": \"S\",\n",
    "        \"ü\": \"u\", \"Ü\": \"U\",\n",
    "    }\n",
    "    return t_no_diac.translate(str.maketrans(tr_map))\n",
    "\n",
    "def fold_key(s: str) -> str:\n",
    "    \"\"\"Fuzzy eşleştirme için diakritik/işaretlerden arındırılmış karşılaştırma anahtarı.\"\"\"\n",
    "    s = normalize_basic(s)\n",
    "    s_norm = unicodedata.normalize(\"NFD\", s)\n",
    "    s_no_diac = \"\".join(ch for ch in s_norm if unicodedata.category(ch) != \"Mn\")\n",
    "    s_no_diac = s_no_diac.replace(\"ı\", \"i\")  # karşılaştırma anahtarında ı->i\n",
    "    return std_space(s_no_diac)\n",
    "\n",
    "def split_multi_cell(x: str) -> list:\n",
    "    \"\"\"Virgül/; / / | ile ayrılmış çoklu değerleri parçala.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x)\n",
    "    parts = re.split(r\"[;,/|]\", s)\n",
    "    parts = [p.strip() for p in parts if p and p.strip()]\n",
    "    return parts\n",
    "\n",
    "def combine_tokens(tokens: list) -> str:\n",
    "    \"\"\"Tekilleştir, sırala ve ', ' ile birleştir.\"\"\"\n",
    "    toks = sorted(set([t for t in tokens if t]))\n",
    "    return \", \".join(toks) if toks else np.nan\n",
    "\n",
    "def build_canonical(series: pd.Series, is_multi: bool, min_count: int) -> list:\n",
    "    \"\"\"Seriden (normalize edilmiş) aday terimleri say ve kanonik küme oluştur.\"\"\"\n",
    "    cnt = Counter()\n",
    "    if is_multi:\n",
    "        for val in series.dropna().astype(str).values:\n",
    "            for t in split_multi_cell(val):\n",
    "                t = normalize_basic(t)\n",
    "                if t:\n",
    "                    cnt[t] += 1\n",
    "    else:\n",
    "        for val in series.dropna().astype(str).values:\n",
    "            t = normalize_basic(val)\n",
    "            if t:\n",
    "                cnt[t] += 1\n",
    "    canonical = [t for t, c in cnt.most_common() if c >= min_count]\n",
    "    return canonical\n",
    "\n",
    "def make_mapper(canonical_terms: list, manual_alias: dict | None, fuzzy_threshold: int):\n",
    "    \"\"\"Token -> kanonik eşleyici döndürür. Önce manuel, sonra fuzzy. Çıkış ASCII olabilir.\"\"\"\n",
    "    canon_keys = [fold_key(c) for c in canonical_terms]\n",
    "    canon_lookup = dict(zip(canon_keys, canonical_terms))  # key -> canonical (lower-case)\n",
    "    \n",
    "    manual_alias = manual_alias or {}\n",
    "    manual_norm = {normalize_basic(k): normalize_basic(v) for k, v in manual_alias.items()}\n",
    "\n",
    "    def _finish(token: str) -> str:\n",
    "        return to_ascii(token) if OUTPUT_ASCII else token\n",
    "\n",
    "    def map_one(token: str) -> str:\n",
    "        if not token:\n",
    "            return \"\"\n",
    "        t_norm = normalize_basic(token)\n",
    "        if not t_norm:\n",
    "            return \"\"\n",
    "        # 1) Manuel alias\n",
    "        if t_norm in manual_norm:\n",
    "            return _finish(manual_norm[t_norm])\n",
    "        # 2) Direkt kanonikte varsa\n",
    "        if t_norm in canonical_terms:\n",
    "            return _finish(t_norm)\n",
    "        # 3) Fuzzy en yakın kanonik\n",
    "        if canon_keys:\n",
    "            key = fold_key(t_norm)\n",
    "            match = process.extractOne(key, canon_keys, scorer=fuzz.token_sort_ratio)\n",
    "            if match:\n",
    "                best_key, score, _ = match\n",
    "                if score >= fuzzy_threshold:\n",
    "                    return _finish(canon_lookup[best_key])\n",
    "        # 4) Eşik altı ise normalize edilmiş haliyle bırak\n",
    "        return _finish(t_norm)\n",
    "\n",
    "    return map_one\n",
    "\n",
    "def standardize_column(series: pd.Series, is_multi: bool, min_count: int, manual_alias: dict | None) -> pd.Series:\n",
    "    \"\"\"Kolonu kanonize eder ve *_std serisi döndürür (çıktı ASCII olabilir).\"\"\"\n",
    "    canonical_terms = build_canonical(series, is_multi=is_multi, min_count=min_count)\n",
    "    mapper = make_mapper(canonical_terms, manual_alias, FUZZY_THRESHOLD)\n",
    "\n",
    "    if is_multi:\n",
    "        out = []\n",
    "        for val in series.values:\n",
    "            tokens = split_multi_cell(val) if pd.notna(val) else []\n",
    "            mapped = [mapper(t) for t in tokens]\n",
    "            out.append(combine_tokens(mapped))\n",
    "        return pd.Series(out, index=series.index, dtype=\"object\")\n",
    "    else:\n",
    "        return series.apply(lambda x: mapper(x) if pd.notna(x) else np.nan)\n",
    "\n",
    "# --------- Ana akış ---------\n",
    "def run_standardization(\n",
    "    df: pd.DataFrame,\n",
    "    cols_multi=COLS_MULTI,\n",
    "    cols_single=COLS_SINGLE,\n",
    "    min_count_default=MIN_COUNT_DEFAULT,\n",
    "    manual_aliases=MANUAL_ALIASES\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Orijinal df + *_std kolonlarıyla birlikte döner (orijinal kolon sırası korunur).\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Çoklu kolonlar\n",
    "    for col in cols_multi:\n",
    "        if col in df.columns:\n",
    "            aliases = manual_aliases.get(col.lower(), {})\n",
    "            df[col + \"_std\"] = standardize_column(\n",
    "                df[col],\n",
    "                is_multi=True,\n",
    "                min_count=min_count_default,\n",
    "                manual_alias=aliases\n",
    "            )\n",
    "\n",
    "    # Tek kolonlar (şu an boş)\n",
    "    for col in cols_single:\n",
    "        if col in df.columns:\n",
    "            aliases = manual_aliases.get(col.lower(), {})\n",
    "            df[col + \"_std\"] = standardize_column(\n",
    "                df[col],\n",
    "                is_multi=False,\n",
    "                min_count=min_count_default,\n",
    "                manual_alias=aliases\n",
    "            )\n",
    "\n",
    "    return df\n",
    "\n",
    "def make_clean_copy_keep_order(df_original: pd.DataFrame,\n",
    "                               df_std: pd.DataFrame,\n",
    "                               cols_multi=COLS_MULTI,\n",
    "                               cols_single=COLS_SINGLE) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Kolon SIRASINI KORUYARAK temiz kopya üretir:\n",
    "      - Orijinal kolon adları ve POZİSYONLARI aynı kalır.\n",
    "      - Hedef kolonların (multi+single) İÇERİĞİ, *_std karşılıklarıyla yerinde değiştirilir.\n",
    "      - *_std kolonları clean kopyaya eklenmez.\n",
    "    \"\"\"\n",
    "    df_clean = df_original.copy()\n",
    "\n",
    "    for col in cols_multi + cols_single:\n",
    "        std_col = col + \"_std\"\n",
    "        if (col in df_clean.columns) and (std_col in df_std.columns):\n",
    "            df_clean[col] = df_std[std_col]\n",
    "\n",
    "    # Clean kopyada *_std kolonları olmasın\n",
    "    std_cols = [c for c in df_clean.columns if c.endswith(\"_std\")]\n",
    "    if std_cols:\n",
    "        df_clean = df_clean.drop(columns=std_cols)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "# --------- Çalıştırma ---------\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_excel(DATA_PATH, sheet_name=SHEET_NAME)\n",
    "\n",
    "    # 1) Orijinal + *_std sütunlarını üret\n",
    "    df_std = run_standardization(\n",
    "        df,\n",
    "        cols_multi=COLS_MULTI,\n",
    "        cols_single=COLS_SINGLE,\n",
    "        min_count_default=MIN_COUNT_DEFAULT,\n",
    "        manual_aliases=MANUAL_ALIASES\n",
    "    )\n",
    "\n",
    "    # 2) Temiz kopya: kolon isimleri ve POZİSYONLARI aynen korunur; içerikler std (ASCII) ile değiştirilir\n",
    "    df_clean = make_clean_copy_keep_order(df_original=df, df_std=df_std,\n",
    "                                          cols_multi=COLS_MULTI, cols_single=COLS_SINGLE)\n",
    "\n",
    "    # ---- Kısa kontrol ----\n",
    "    print(\"\\n[Kontrol] df kolon sırası korunuyor mu?\")\n",
    "    print(\"Aynı mı:\", list(df.columns) == list(df_clean.columns))\n",
    "    print(\"Standardize edilen kolonlar:\", COLS_MULTI + COLS_SINGLE)\n",
    "    print(\"ASCII çıktı aktif mi?:\", OUTPUT_ASCII)\n",
    "\n",
    "    # 3) Kaydet\n",
    "    with pd.ExcelWriter(OUT_STD_ALL, engine=\"openpyxl\") as writer:\n",
    "        df_std.to_excel(writer, index=False)\n",
    "    with pd.ExcelWriter(OUT_CLEAN, engine=\"openpyxl\") as writer:\n",
    "        df_clean.to_excel(writer, index=False)\n",
    "\n",
    "    print(f\"\\n✅ Kaydedildi:\")\n",
    "    print(f\" - {OUT_STD_ALL}  (orijinal + *_std)\")\n",
    "    print(f\" - {OUT_CLEAN}    (clean: kolon sırası AYNI, içerik standardize, ASCII)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31e3f723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T20:06:38.352021Z",
     "iopub.status.busy": "2025-09-05T20:06:38.351495Z",
     "iopub.status.idle": "2025-09-05T20:06:42.612613Z",
     "shell.execute_reply": "2025-09-05T20:06:42.611335Z"
    },
    "papermill": {
     "duration": 4.26989,
     "end_time": "2025-09-05T20:06:42.614392",
     "exception": false,
     "start_time": "2025-09-05T20:06:38.344502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rows_in': 2235, 'dropped_whole_body': 4, 'dropped_duration_5': 351, 'rows_out': 511}\n",
      "✅ Kaydedildi:\n",
      " - /kaggle/working/Clean_Data_Case_DT_2025_clean_final.xlsx\n",
      " - /kaggle/working/Clean_Data_Case_DT_2025_clean_final.csv\n"
     ]
    }
   ],
   "source": [
    "# === Clean & Deduplicate Pipeline (ENTEGRE + demografik doldurma + taraf uyumu + GRUP ANAHTARI) ===\n",
    "# Girdi  -> /kaggle/working/Clean_Data_Case_DT_2025_clean.xlsx\n",
    "# Çıktı  -> /kaggle/working/Clean_Data_Case_DT_2025_clean_final.(xlsx|csv)\n",
    "\n",
    "import re, numpy as np, pandas as pd\n",
    "\n",
    "IN_PATH  = \"/kaggle/working/Clean_Data_Case_DT_2025_clean.xlsx\"\n",
    "OUT_XLSX = \"/kaggle/working/Clean_Data_Case_DT_2025_clean_final.xlsx\"\n",
    "OUT_CSV  = \"/kaggle/working/Clean_Data_Case_DT_2025_clean_final.csv\"\n",
    "\n",
    "# (Opsiyonel) 5 dakikalık süre anomali filtresi bayrağı\n",
    "APPLY_DROP_5 = True\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def to_int_safe(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    m = re.search(r\"(\\d+)\", str(x))\n",
    "    return int(m.group(1)) if m else np.nan\n",
    "\n",
    "def tidy_trailing_punct(s):\n",
    "    if pd.isna(s): return s\n",
    "    t = str(s).strip()\n",
    "    t = re.sub(r\"[,\\s]+$\", \"\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t.strip()\n",
    "\n",
    "def split_tokens(s):\n",
    "    if pd.isna(s) or str(s).strip()==\"\":\n",
    "        return []\n",
    "    return [p.strip() for p in re.split(r\"[;,]\", str(s)) if p and p.strip()]\n",
    "\n",
    "def join_tokens(tokens):\n",
    "    toks = sorted(set([t for t in tokens if t]))\n",
    "    return \", \".join(toks) if toks else np.nan\n",
    "\n",
    "def mode_or_first(series: pd.Series):\n",
    "    vc = series.dropna().astype(str).value_counts()\n",
    "    if vc.empty: return np.nan\n",
    "    top = vc.max()\n",
    "    cands = sorted(vc[vc==top].index.tolist())\n",
    "    return cands[0]\n",
    "\n",
    "def collapse_hyphen_repeat(s):\n",
    "    if pd.isna(s): return s\n",
    "    t = str(s)\n",
    "    if \"-\" in t:\n",
    "        parts = [p.strip() for p in t.split(\"-\") if p.strip()]\n",
    "        if len(parts)==2 and parts[0].lower()==parts[1].lower():\n",
    "            return parts[0]\n",
    "    return t\n",
    "\n",
    "# ---------------- 1) Load ----------------\n",
    "df = pd.read_excel(IN_PATH)\n",
    "rows_in = len(df)\n",
    "cols_order = df.columns.tolist()\n",
    "\n",
    "# snake_case isimler (varsa dönüştür)\n",
    "rename_map = {\n",
    "    'HastaNo':'hasta_no','Yas':'yas','Cinsiyet':'cinsiyet','KanGrubu':'kan_grubu','Uyruk':'uyruk',\n",
    "    'KronikHastalik':'kronik_hastalik','Bolum':'bolum','Alerji':'alerji','Tanilar':'tanilar',\n",
    "    'TedaviAdi':'tedavi_adi','TedaviSuresi':'tedavi_suresi',\n",
    "    'UygulamaYerleri':'uygulama_yerleri','UygulamaSuresi':'uygulama_suresi'\n",
    "}\n",
    "df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns}, inplace=True)\n",
    "\n",
    "# ---------------- 2) Aynı hasta içinde boşları doldur (demografik + kronik) ----------------\n",
    "def fill_by_patient_mode(df, patient_col=\"hasta_no\",\n",
    "                         cols_to_fill=(\"kronik_hastalik\",\"cinsiyet\",\"kan_grubu\",\"uyruk\",\"yas\")):\n",
    "    df = df.copy()\n",
    "    if patient_col not in df.columns:\n",
    "        return df\n",
    "    g = df.groupby(patient_col, dropna=False)\n",
    "    for col in cols_to_fill:\n",
    "        if col not in df.columns: \n",
    "            continue\n",
    "        mode_map = g[col].transform(mode_or_first)\n",
    "        mask_na = df[col].isna() | (df[col].astype(str).str.strip()==\"\")\n",
    "        df.loc[mask_na, col] = mode_map[mask_na]\n",
    "    return df\n",
    "\n",
    "df = fill_by_patient_mode(df)\n",
    "\n",
    "# ---------------- 3) Kozmetik temizlik ----------------\n",
    "if \"tedavi_adi\" in df.columns:\n",
    "    df[\"tedavi_adi\"] = df[\"tedavi_adi\"].apply(tidy_trailing_punct).apply(collapse_hyphen_repeat)\n",
    "\n",
    "msk_pat = r\"(men[iı]sk[üu]s|diz|kal(c|ç)a|omuz|aş[iı]l|ayak.*bile|bilek|dirsek|el|sirt|boyun|trokanter)\"\n",
    "if \"bolum\" in df.columns:\n",
    "    tanilar_l = df.get(\"tanilar\", pd.Series([\"\"]*len(df))).fillna(\"\").str.lower()\n",
    "    tedavi_l  = df.get(\"tedavi_adi\", pd.Series([\"\"]*len(df))).fillna(\"\").str.lower()\n",
    "    is_msk = tanilar_l.str.contains(msk_pat, regex=True) | tedavi_l.str.contains(msk_pat, regex=True)\n",
    "    def drop_solunum_in_msk(val, is_msk_flag):\n",
    "        if not is_msk_flag or pd.isna(val): \n",
    "            return val\n",
    "        toks = split_tokens(val)\n",
    "        toks = [t for t in toks if t.strip().lower() != \"solunum merkezi\"]\n",
    "        return join_tokens(toks) if toks else val\n",
    "    df[\"bolum\"] = [drop_solunum_in_msk(v, flag) for v, flag in zip(df[\"bolum\"], is_msk)]\n",
    "\n",
    "if \"uygulama_yerleri\" in df.columns:\n",
    "    df[\"uygulama_yerleri\"] = df[\"uygulama_yerleri\"].apply(tidy_trailing_punct)\n",
    "\n",
    "# ---------------- 3.45) tedavi_adi'ndaki taraf bilgisini uygulama_yerleri ile hizala ----------------\n",
    "SIDE_SAG = re.compile(r\"\\b(sağ|sag)\\b\", re.I)\n",
    "SIDE_SOL = re.compile(r\"\\bsol\\b\", re.I)\n",
    "ANAT_PAT = re.compile(r\"\\b(diz|kal(c|ç)a|ayak(?:\\s*|.*)bilek|bilek|omuz|dirsek|el|ayak|bacak|uyluk)\\b\", re.I)\n",
    "\n",
    "def detect_side(text):\n",
    "    if not isinstance(text, str): \n",
    "        return None\n",
    "    t = text.lower()\n",
    "    if SIDE_SOL.search(t): return \"sol\"\n",
    "    if SIDE_SAG.search(t): return \"sag\"\n",
    "    return None\n",
    "\n",
    "def enforce_side_on_site(site, target_side):\n",
    "    if pd.isna(site) or not target_side:\n",
    "        return site\n",
    "    s = str(site)\n",
    "    s_low = s.lower()\n",
    "    if \"bilateral\" in s_low:\n",
    "        return s\n",
    "    if target_side == \"sol\":\n",
    "        s2 = re.sub(r\"\\b(sağ|sag)\\b\", \"sol\", s, flags=re.I)\n",
    "    else:  # \"sag\"\n",
    "        s2 = re.sub(r\"\\bsol\\b\", \"sag\", s, flags=re.I)\n",
    "    s2_low = s2.lower()\n",
    "    if not (SIDE_SOL.search(s2_low) or SIDE_SAG.search(s2_low)):\n",
    "        if ANAT_PAT.search(s2_low):\n",
    "            s2 = f\"{target_side} {s2}\"\n",
    "    s2 = re.sub(r\"\\s*,\\s*\", \", \", s2).strip()\n",
    "    return s2\n",
    "\n",
    "if {\"tedavi_adi\",\"uygulama_yerleri\"}.issubset(df.columns):\n",
    "    target_side_from_tedavi = df[\"tedavi_adi\"].astype(str).apply(detect_side)\n",
    "    mask_need_side = target_side_from_tedavi.notna()\n",
    "    df.loc[mask_need_side, \"uygulama_yerleri\"] = [\n",
    "        enforce_side_on_site(site, side) \n",
    "        for site, side in zip(df.loc[mask_need_side, \"uygulama_yerleri\"], \n",
    "                              target_side_from_tedavi[mask_need_side])\n",
    "    ]\n",
    "\n",
    "# ---------------- 3.4) (YENİ) GRUP ANAHTARI normalizasyonu ----------------\n",
    "TURK_MAP = str.maketrans({\n",
    "    \"ç\":\"c\",\"Ç\":\"c\",\"ğ\":\"g\",\"Ğ\":\"g\",\"ı\":\"i\",\"İ\":\"i\",\"I\":\"i\",\n",
    "    \"ö\":\"o\",\"Ö\":\"o\",\"ş\":\"s\",\"Ş\":\"s\",\"ü\":\"u\",\"Ü\":\"u\"\n",
    "})\n",
    "EXCLUDE_FOR_GROUP = {\"uygulama_yerleri\", \"uygulama_suresi\"}  # gruptan hariç\n",
    "TEXT_COLS_FOR_KEY = [c for c in df.columns if c not in EXCLUDE_FOR_GROUP]\n",
    "\n",
    "def norm_key(v):\n",
    "    if pd.isna(v): return \"\"\n",
    "    t = str(v).lower().translate(TURK_MAP)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    t = re.sub(r\"[\\W_]+\", \"\", t)\n",
    "    return t\n",
    "\n",
    "for c in TEXT_COLS_FOR_KEY:\n",
    "    df[f\"_gk_{c}\"] = df[c].apply(norm_key)\n",
    "\n",
    "GK_COLS = [f\"_gk_{c}\" for c in TEXT_COLS_FOR_KEY]\n",
    "\n",
    "# ---------------- 3.5) Hasta+Tedavi bazında uygulama_yerleri’ni tutarlılaştır ----------------\n",
    "WHOLE_BODY = re.compile(r\"\\bt[üu]m v[üu]cut\\b\", flags=re.I)\n",
    "SPECIFIC   = re.compile(r\"\\b(diz|ayak.*bile|bilek|kal(c|ç)a|omuz|bel|boyun|dirsek|el|s[ıi]rt|trapez|skapula)\\b\", re.I)\n",
    "\n",
    "def classify_site(text):\n",
    "    if text is None or str(text).strip()==\"\":\n",
    "        return 0\n",
    "    t = str(text).strip().lower()\n",
    "    if WHOLE_BODY.search(t): return 1\n",
    "    if SPECIFIC.search(t):   return 2\n",
    "    return 1\n",
    "\n",
    "def split_sites(s):\n",
    "    if pd.isna(s) or str(s).strip()==\"\":\n",
    "        return []\n",
    "    return sorted({p.strip() for p in re.split(r\"[;,]\", str(s)) if p and p.strip()})\n",
    "\n",
    "def choose_representative_site(series):\n",
    "    s = series.fillna(\"\").astype(str)\n",
    "    if s.empty: return np.nan\n",
    "    cls = s.apply(classify_site)\n",
    "    max_cls = cls.max()\n",
    "    cand = s[cls == max_cls]\n",
    "    if max_cls == 2:\n",
    "        records = [(len(split_sites(v)), v) for v in cand]\n",
    "        max_size = max(sz for sz,_ in records)\n",
    "        cand2 = [v for sz,v in records if sz == max_size]\n",
    "        vc = pd.Series(cand2).value_counts()\n",
    "        top = vc.max()\n",
    "        cands = sorted(vc[vc==top].index.tolist())\n",
    "        return cands[0]\n",
    "    else:\n",
    "        vc = cand.value_counts()\n",
    "        top = vc.max()\n",
    "        cands = sorted(vc[vc==top].index.tolist())\n",
    "        return cands[0] if cands else np.nan\n",
    "\n",
    "group_keys_ht = [c for c in [\"hasta_no\",\"tedavi_adi\"] if c in df.columns]\n",
    "if group_keys_ht:\n",
    "    rep_site = df.groupby(group_keys_ht)[\"uygulama_yerleri\"].transform(choose_representative_site)\n",
    "    is_whole = df[\"uygulama_yerleri\"].fillna(\"\").str.lower().str.contains(WHOLE_BODY)\n",
    "    is_empty = df[\"uygulama_yerleri\"].isna() | (df[\"uygulama_yerleri\"].astype(str).str.strip().eq(\"\"))\n",
    "    need_fill = is_whole | is_empty\n",
    "    df.loc[need_fill, \"uygulama_yerleri\"] = rep_site[need_fill]\n",
    "\n",
    "# ---------------- 4) 'Tüm Vücut' saçmalığını at (spesifik varsa) [GK_COLS ile grup] ----------------\n",
    "def drop_whole_body_if_specific_exists(df):\n",
    "    df = df.copy()\n",
    "    if \"uygulama_yerleri\" not in df.columns: \n",
    "        return df, 0\n",
    "    specific_pat = SPECIFIC.pattern\n",
    "    whole_body_pat = WHOLE_BODY.pattern\n",
    "    grp = df.groupby(GK_COLS, dropna=False)\n",
    "    has_specific = grp[\"uygulama_yerleri\"].transform(\n",
    "        lambda s: s.fillna(\"\").str.lower().str.contains(specific_pat, regex=True).any()\n",
    "    )\n",
    "    is_whole = df[\"uygulama_yerleri\"].fillna(\"\").str.lower().str.contains(whole_body_pat, regex=True)\n",
    "    drop_idx = df.index[has_specific & is_whole]\n",
    "    return df.drop(index=drop_idx), len(drop_idx)\n",
    "\n",
    "df, dropped_whole = drop_whole_body_if_specific_exists(df)\n",
    "\n",
    "# ---------------- 5) Süre 5 anomalisini at (grupta >5 varsa) [GK_COLS ile grup] ----------------\n",
    "def drop_duration_5_anomaly(df):\n",
    "    df = df.copy()\n",
    "    if \"uygulama_suresi\" not in df.columns:\n",
    "        return df, 0\n",
    "    df[\"_dur\"] = df[\"uygulama_suresi\"].apply(to_int_safe)\n",
    "    grp = df.groupby(GK_COLS, dropna=False)\n",
    "    has_gt5 = grp[\"_dur\"].transform(lambda s: (s > 5).any())\n",
    "    mask_drop5 = has_gt5 & (df[\"_dur\"] == 5)\n",
    "    out = df.loc[~mask_drop5].copy()\n",
    "    out.drop(columns=[\"_dur\"], inplace=True, errors=\"ignore\")\n",
    "    return out, int(mask_drop5.sum())\n",
    "\n",
    "if APPLY_DROP_5:\n",
    "    df, dropped_5 = drop_duration_5_anomaly(df)\n",
    "else:\n",
    "    dropped_5 = 0\n",
    "\n",
    "# ---------------- 6) Tekilleştirme (grup başına tek satır) [GK_COLS ile grup] ----------------\n",
    "# >>> GÜNCELLENEN, DAHA GÜVENLİ SÜRÜM ( _dur_diff OLUŞTURMADAN ) <<<\n",
    "def collapse_groups_keep_superset_and_mode(df):\n",
    "    \"\"\"\n",
    "    GK_COLS aynı olan kayıtları tekilleştirir.\n",
    "    - Uygulama yeri: en kapsamlı (token sayısı en çok) tercih edilir.\n",
    "    - Süre: grubun modu (eşitlikte maksimum) 'temsilî süre' olarak alınır.\n",
    "    - En kapsamlı adaylarda temsilî süre yoksa:\n",
    "        * en kapsamlılar içinden rep_dur'a |dur-rep_dur| EN YAKIN olanı seç,\n",
    "        * çıktıdaki 'uygulama_suresi'ni rep_dur'a normalize et.\n",
    "    \"\"\"\n",
    "    def _split_sites(s):\n",
    "        if pd.isna(s) or str(s).strip()==\"\":\n",
    "            return []\n",
    "        return sorted({p.strip() for p in re.split(r\"[;,]\", str(s)) if p and p.strip()})\n",
    "\n",
    "    def pick_rep_duration(s: pd.Series) -> float:\n",
    "        vc = s.value_counts(dropna=True)\n",
    "        if vc.empty: return np.nan\n",
    "        top = vc.max()\n",
    "        cands = vc[vc==top].index.tolist()\n",
    "        return float(np.nanmax(cands))  # eşitlikte maksimum\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"_dur\"] = df[\"uygulama_suresi\"].apply(to_int_safe).astype(\"float\") if \"uygulama_suresi\" in df.columns else np.nan\n",
    "    df[\"_sites\"] = df.get(\"uygulama_yerleri\", pd.Series([np.nan]*len(df))).apply(_split_sites)\n",
    "    df[\"_sites_size\"] = df[\"_sites\"].apply(len).astype(int)\n",
    "\n",
    "    rows = []\n",
    "    for _, gdf in df.groupby(GK_COLS, dropna=False):\n",
    "        gdf = gdf.copy()\n",
    "        rep_dur = pick_rep_duration(gdf[\"_dur\"]) if \"_dur\" in gdf else np.nan\n",
    "\n",
    "        max_site_sz = gdf[\"_sites_size\"].max()\n",
    "        g_best = gdf[gdf[\"_sites_size\"] == max_site_sz]\n",
    "\n",
    "        # 1) En kapsamlı + rep_dur eşleşmesi varsa, onu seç\n",
    "        cand = g_best[g_best[\"_dur\"] == rep_dur]\n",
    "        if len(cand) == 0:\n",
    "            # 2) En kapsamlılar içinde rep_dur'a EN YAKIN süreyi seç (kolon eklemeden)\n",
    "            if not np.isnan(rep_dur):\n",
    "                diffs = (g_best[\"_dur\"] - rep_dur).abs()\n",
    "                # Eşitlikte küçük süreyi tercih etmek için stable sort (mergesort) + ikincil anahtar\n",
    "                order = np.lexsort((g_best[\"_dur\"].values, diffs.values))\n",
    "                idx = g_best.index[order][0]\n",
    "                cand = g_best.loc[[idx]]\n",
    "            else:\n",
    "                cand = g_best.head(1)\n",
    "\n",
    "        if len(cand) == 0:\n",
    "            cand = gdf[gdf[\"_dur\"] == rep_dur]\n",
    "            if len(cand) == 0:\n",
    "                cand = gdf.head(1)\n",
    "\n",
    "        row = cand.iloc[0].copy()\n",
    "        if not np.isnan(rep_dur):\n",
    "            row[\"uygulama_suresi\"] = int(rep_dur)\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    out = pd.DataFrame(rows).copy()\n",
    "    # Yardımcı kolonları (ve varsa yanlışlıkla sızan _dur_diff'i) temizle\n",
    "    out.drop(columns=[\"_dur\",\"_sites\",\"_sites_size\",\"_dur_diff\"], inplace=True, errors=\"ignore\")\n",
    "    return out\n",
    "\n",
    "df_final = collapse_groups_keep_superset_and_mode(df)\n",
    "\n",
    "# ---------------- 7) Save ----------------\n",
    "rows_out = len(df_final)\n",
    "print({\n",
    "    \"rows_in\": rows_in,\n",
    "    \"dropped_whole_body\": dropped_whole,\n",
    "    \"dropped_duration_5\": dropped_5,\n",
    "    \"rows_out\": rows_out\n",
    "})\n",
    "\n",
    "# Kolon sırasını koru ve _gk_* yardımcılarını at\n",
    "for c in df_final.columns:\n",
    "    if c not in cols_order:\n",
    "        cols_order.append(c)\n",
    "df_final = df_final[[c for c in cols_order if c in df_final.columns]]\n",
    "df_final.drop(columns=[c for c in df_final.columns if c.startswith(\"_gk_\")],\n",
    "              inplace=True, errors=\"ignore\")\n",
    "\n",
    "df_final.to_excel(OUT_XLSX, index=False)\n",
    "df_final.to_csv(OUT_CSV, index=False)\n",
    "print(\"✅ Kaydedildi:\")\n",
    "print(\" -\", OUT_XLSX)\n",
    "print(\" -\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9730de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T20:06:42.627659Z",
     "iopub.status.busy": "2025-09-05T20:06:42.627310Z",
     "iopub.status.idle": "2025-09-05T20:06:42.955309Z",
     "shell.execute_reply": "2025-09-05T20:06:42.953863Z"
    },
    "papermill": {
     "duration": 0.337509,
     "end_time": "2025-09-05T20:06:42.958037",
     "exception": false,
     "start_time": "2025-09-05T20:06:42.620528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKUNAN: /kaggle/working/Clean_Data_Case_DT_2025_clean_final.xlsx\n",
      "Şekil: (511, 13)\n",
      "\n",
      "[CINSIYET DAĞILIMI]\n",
      "cinsiyet\n",
      "Kadın    304\n",
      "Erkek    207\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Kaydedildi:\n",
      "/kaggle/working/dataset_gender_filled.csv\n",
      "/kaggle/working/dataset_gender_filled.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cinsiyet Doldurma + Boşluk Temizleme (Kaggle)\n",
    "# ============================\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- YOLLAR ----\n",
    "# Excel/CSV her ikisini de destekleyelim; varsa Excel'i, yoksa CSV'yi oku.\n",
    "PREFERRED = [\n",
    "    \"/kaggle/working/Clean_Data_Case_DT_2025_clean_final.xlsx\",\n",
    "    \"/kaggle/working/Clean_Data_Case_DT_2025_clean_final.csv\",\n",
    "]\n",
    "\n",
    "def _read_any(preferred_paths):\n",
    "    for p in preferred_paths:\n",
    "        try:\n",
    "            if p.lower().endswith(\".xlsx\"):\n",
    "                return pd.read_excel(p), p\n",
    "            elif p.lower().endswith(\".csv\"):\n",
    "                return pd.read_csv(p), p\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise FileNotFoundError(\"Okunabilir bir dosya bulunamadı. Yolu güncelleyin.\")\n",
    "\n",
    "df, in_path = _read_any(PREFERRED)\n",
    "print(\"OKUNAN:\", in_path)\n",
    "print(\"Şekil:\", df.shape)\n",
    "\n",
    "# ---- Yardımcılar ----\n",
    "def _is_na(x):\n",
    "    if x is None:\n",
    "        return True\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"nan\", \"none\", \"null\"}:\n",
    "        return True\n",
    "    return pd.isna(x)\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"Türkçe karakterleri koruyarak sadece küçük harfe çeker ve boşlukları sıkıştırır.\"\"\"\n",
    "    if _is_na(s): \n",
    "        return \"\"\n",
    "    s = str(s).casefold()\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# ---- Kolon isimleri (dosyan göre uyumlu) ----\n",
    "COL_CINS = \"cinsiyet\"\n",
    "COL_KRON = \"kronik_hastalik\"\n",
    "COL_TANI = \"tanilar\"\n",
    "COL_ALER = \"alerji\"\n",
    "\n",
    "missing_cols = [c for c in [COL_CINS, COL_KRON, COL_TANI, COL_ALER] if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Beklenen kolon(lar) yok: {missing_cols}\")\n",
    "\n",
    "# ===========================================\n",
    "# 1) Alerji & Kronik Hastalık NaN -> \"yok\"\n",
    "# ===========================================\n",
    "for col in [COL_ALER, COL_KRON]:\n",
    "    df[col] = df[col].apply(lambda x: \"yok\" if _is_na(x) else str(x).strip())\n",
    "\n",
    "# ===========================================\n",
    "# 2) Cinsiyet: Kural Tabanlı + Oransal Doldurma\n",
    "# ===========================================\n",
    "# --- Kural kümeleri (genişletilebilir) ---\n",
    "# Kadın için daha ayırt edici ipuçları:\n",
    "female_patterns = [\n",
    "    r\"\\b(rahim|uterus|serviks|servical|servicalis)\\b\",\n",
    "    r\"\\b(over|ovary|ovaryan|ovarian|over kisti|polikistik over|pko)\\b\",\n",
    "    r\"\\b(meme|endometrioz|endometriozis)\\b\",\n",
    "    r\"\\b(gebelik|hamile|gravid|lohusalık|menopoz|adet|dismenore)\\b\",\n",
    "    r\"\\b(tiroid|hipotiroid|hipertiroid|tiroidit)\\b\",\n",
    "    r\"\\b(myasteni|myasthenia)\\b\",\n",
    "    r\"\\b(polimiyozit|polimiyozit)\\b\",\n",
    "]\n",
    "\n",
    "# Erkek için daha ayırt edici ipuçları:\n",
    "male_patterns = [\n",
    "    r\"\\b(prostat|bph|benign prostat hiperplazisi)\\b\",\n",
    "    r\"\\b(testis|testik|penil|epididim|varikosel|erkek infertil)\\b\",\n",
    "    r\"\\b(hipospad|hypospad)\\b\",\n",
    "    r\"\\b(duchenne|becker)\\b\",  # musküler distrofi alt tipleri\n",
    "]\n",
    "\n",
    "female_re = re.compile(\"|\".join(female_patterns))\n",
    "male_re   = re.compile(\"|\".join(male_patterns))\n",
    "\n",
    "# --- Cinsiyeti normalize et (sadece 'Kadın'/'Erkek' kalsın) ---\n",
    "def _std_gender(x):\n",
    "    if _is_na(x): \n",
    "        return np.nan\n",
    "    s = str(x).strip().casefold()\n",
    "    if s in {\"kadin\", \"kadın\", \"female\", \"f\", \"k\", \"woman\"}:\n",
    "        return \"Kadın\"\n",
    "    if s in {\"erkek\", \"male\", \"m\", \"e\", \"man\"}:\n",
    "        return \"Erkek\"\n",
    "    return np.nan\n",
    "\n",
    "df[COL_CINS] = df[COL_CINS].apply(_std_gender)\n",
    "\n",
    "# --- Kural tabanlı infer: sadece boş cinsiyetler için uygula ---\n",
    "mask_missing = df[COL_CINS].isna()\n",
    "\n",
    "# Tanı + kronik birleştirip normalize et\n",
    "comb_text = (\n",
    "    df[COL_KRON].fillna(\"\").astype(str) + \" \" + df[COL_TANI].fillna(\"\").astype(str)\n",
    ").apply(_norm)\n",
    "\n",
    "def infer_gender_from_text(txt: str):\n",
    "    if not txt:\n",
    "        return np.nan\n",
    "    has_f = bool(female_re.search(txt))\n",
    "    has_m = bool(male_re.search(txt))\n",
    "    if has_f and not has_m:\n",
    "        return \"Kadın\"\n",
    "    if has_m and not has_f:\n",
    "        return \"Erkek\"\n",
    "    # Çakışma veya ipucu yoksa boş bırak (sonra oransal doldurulacak)\n",
    "    return np.nan\n",
    "\n",
    "df.loc[mask_missing, COL_CINS] = comb_text[mask_missing].apply(infer_gender_from_text)\n",
    "\n",
    "# --- Geriye kalan boşları oransal doldur ---\n",
    "mask_missing = df[COL_CINS].isna()\n",
    "if mask_missing.any():\n",
    "    vc = df[COL_CINS].value_counts(dropna=True)\n",
    "    n_known = vc.sum()\n",
    "    if n_known == 0:\n",
    "        p_female = 0.5\n",
    "        p_male   = 0.5\n",
    "    else:\n",
    "        p_female = float(vc.get(\"Kadın\", 0)) / n_known\n",
    "        p_male   = float(vc.get(\"Erkek\", 0)) / n_known\n",
    "        # güvenlik: oranlardan biri 0 ise çok küçük bir epsilon ekleyelim\n",
    "        eps = 1e-9\n",
    "        p_female = max(p_female, eps)\n",
    "        p_male   = max(p_male, eps)\n",
    "        s = p_female + p_male\n",
    "        p_female, p_male = p_female/s, p_male/s\n",
    "\n",
    "    rng = np.random.default_rng(42)  # tekrarlanabilirlik\n",
    "    fill_vals = rng.choice([\"Kadın\", \"Erkek\"], size=mask_missing.sum(), p=[p_female, p_male])\n",
    "    df.loc[mask_missing, COL_CINS] = fill_vals\n",
    "\n",
    "# ============================\n",
    "# Kontrol & Kayıt\n",
    "# ============================\n",
    "print(\"\\n[Cinsiyet Dağılımı]\".upper())\n",
    "print(df[COL_CINS].value_counts())\n",
    "\n",
    "# Çıkış dosyaları\n",
    "out_csv  = \"/kaggle/working/dataset_gender_filled.csv\"\n",
    "out_xlsx = \"/kaggle/working/dataset_gender_filled.xlsx\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "try:\n",
    "    df.to_excel(out_xlsx, index=False)\n",
    "except Exception as e:\n",
    "    print(\"Excel'e yazarken uyarı:\", e)\n",
    "\n",
    "print(\"\\nKaydedildi:\")\n",
    "print(out_csv)\n",
    "print(out_xlsx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6027a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T20:06:42.972122Z",
     "iopub.status.busy": "2025-09-05T20:06:42.971742Z",
     "iopub.status.idle": "2025-09-05T20:06:44.369625Z",
     "shell.execute_reply": "2025-09-05T20:06:44.368149Z"
    },
    "papermill": {
     "duration": 1.406963,
     "end_time": "2025-09-05T20:06:44.371386",
     "exception": false,
     "start_time": "2025-09-05T20:06:42.964423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKUNAN: /kaggle/working/Clean_Data_Case_DT_2025_clean_final.xlsx\n",
      "Şekil: (511, 13)\n",
      "\n",
      "BAŞLANGIÇ NaN SAYILARI:\n",
      "kan_grubu           136\n",
      "bolum                 2\n",
      "tanilar              20\n",
      "uygulama_yerleri     34\n",
      "dtype: int64\n",
      "\n",
      "[Hasta bazlı doldurulan]   {'bolum': 0, 'tanilar': 4, 'uygulama_yerleri': 11}\n",
      "\n",
      "[Grup bazlı doldurulan]    {'bolum': 0, 'tanilar': 0, 'uygulama_yerleri': 0}\n",
      "\n",
      "SON NaN SAYILARI (hepsi 0 olmalı):\n",
      "kan_grubu           0\n",
      "bolum               0\n",
      "tanilar             0\n",
      "uygulama_yerleri    0\n",
      "dtype: int64\n",
      "\n",
      "=== ÖZET ===\n",
      "Hasta bazlı doldurulanlar: {'bolum': 0, 'tanilar': 4, 'uygulama_yerleri': 11}\n",
      "Grup bazlı doldurulanlar : {'bolum': 0, 'tanilar': 0, 'uygulama_yerleri': 0}\n",
      "\n",
      "Top bolum değerleri:\n",
      "bolum\n",
      "Fiziksel Tıp Ve Rehabilitasyon                    313\n",
      "Fiziksel Tıp Ve Rehabilitasyon,Solunum Merkezi    128\n",
      "Ortopedi Ve Travmatoloji                           40\n",
      "İç Hastalıkları                                     9\n",
      "Nöroloji                                            5\n",
      "Kardiyoloji                                         4\n",
      "Genel Cerrahi                                       3\n",
      "bilinmiyor                                          2\n",
      "Tıbbi Onkoloji                                      2\n",
      "Göğüs Hastalıkları                                  2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top uygulama_yerleri değerleri:\n",
      "uygulama_yerleri\n",
      "bel                        104\n",
      "boyun                       69\n",
      "tum vucut bolgesi           47\n",
      "diz                         37\n",
      "sol omuz bolgesi            31\n",
      "bilinmiyor                  23\n",
      "sag omuz bolgesi            23\n",
      "sol el bilek bolgesi        21\n",
      "sag ayak bilegi bolgesi     19\n",
      "sirt                        17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Kaydedildi:\n",
      "/kaggle/working/dataset_imputed.csv\n",
      "/kaggle/working/dataset_imputed.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Güvenli Doldurma Pipeline (Hasta -> Grup -> Bilinmiyor)\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Girdi yolları (varsa hangisi bulunursa onu okuyacak) ----\n",
    "PREFERRED = [\n",
    "    \"/kaggle/working/Clean_Data_Case_DT_2025_clean_final.xlsx\"\n",
    "]\n",
    "\n",
    "def _read_any(paths):\n",
    "    for p in paths:\n",
    "        try:\n",
    "            if p.lower().endswith(\".xlsx\"):\n",
    "                return pd.read_excel(p), p\n",
    "            elif p.lower().endswith(\".csv\"):\n",
    "                return pd.read_csv(p), p\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise FileNotFoundError(\"Okunabilir bir dosya bulunamadı; PREFERRED listesini güncelleyin.\")\n",
    "\n",
    "df, in_path = _read_any(PREFERRED)\n",
    "print(\"OKUNAN:\", in_path)\n",
    "print(\"Şekil:\", df.shape)\n",
    "\n",
    "# ---- Beklenen kolonlar ----\n",
    "REQUIRED = [\n",
    "    \"hasta_no\",\"yas\",\"cinsiyet\",\"kan_grubu\",\"uyruk\",\"kronik_hastalik\",\n",
    "    \"bolum\",\"alerji\",\"tanilar\",\"tedavi_adi\",\"tedavi_suresi\",\n",
    "    \"uygulama_yerleri\",\"uygulama_suresi\"\n",
    "]\n",
    "missing_cols = [c for c in REQUIRED if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Eksik kolon(lar): {missing_cols}\")\n",
    "\n",
    "# ---- Yardımcılar ----\n",
    "def is_na_like(x):\n",
    "    if x is None:\n",
    "        return True\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"nan\",\"none\",\"null\"}:\n",
    "        return True\n",
    "    return pd.isna(x)\n",
    "\n",
    "def safe_mode(series: pd.Series):\n",
    "    \"\"\"En sık değeri döndürür (NaN hariç). Boşsa None.\"\"\"\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return None\n",
    "    return s.mode(dropna=True).iloc[0] if not s.mode(dropna=True).empty else None\n",
    "\n",
    "def unique_if_consistent(series: pd.Series):\n",
    "    \"\"\"Seride NaN hariç tek bir değer varsa onu döndür; yoksa None.\"\"\"\n",
    "    s = series.dropna().unique()\n",
    "    return s[0] if len(s) == 1 else None\n",
    "\n",
    "def fill_from_patient_consensus(df, target_col, id_col=\"hasta_no\"):\n",
    "    \"\"\"\n",
    "    Aynı hasta_no'da target_col için çelişki yoksa (tek değer) eksikleri o değerle doldurur.\n",
    "    \"\"\"\n",
    "    before_missing = df[target_col].isna().sum()\n",
    "    # hasta_no -> değer eşlemesi (tutarlıysa)\n",
    "    cons_map = (\n",
    "        df.groupby(id_col)[target_col]\n",
    "          .apply(unique_if_consistent)\n",
    "          .dropna()\n",
    "          .to_dict()\n",
    "    )\n",
    "    mask = df[target_col].isna() & df[id_col].isin(cons_map.keys())\n",
    "    df.loc[mask, target_col] = df.loc[mask, id_col].map(cons_map)\n",
    "    filled = before_missing - df[target_col].isna().sum()\n",
    "    return int(filled)\n",
    "\n",
    "def fill_from_group_mode(df, target_col, group_cols, min_count=3, min_ratio=0.7, unanimous=False):\n",
    "    \"\"\"\n",
    "    Grup bazlı doldurma.\n",
    "    - mode değer sayısı >= min_count olmalı.\n",
    "    - mode oranı >= min_ratio (ör. 0.7) olmalı.\n",
    "    - unanimous=True ise gruptaki non-null değerlerin tamamı tek değerse doldur.\n",
    "    \"\"\"\n",
    "    before_missing = df[target_col].isna().sum()\n",
    "\n",
    "    grp = df.groupby(group_cols)[target_col]\n",
    "    stats = grp.agg(\n",
    "        non_null_count=lambda s: s.notna().sum(),\n",
    "        nunique=lambda s: s.dropna().nunique(),\n",
    "        mode_val=lambda s: safe_mode(s),\n",
    "    ).reset_index()\n",
    "\n",
    "    if unanimous:\n",
    "        # Tam mutabakat: non-null > 0 ve nunique == 1\n",
    "        ok = stats.query(\"non_null_count >= @min_count and nunique == 1\").copy()\n",
    "        ok[\"use_val\"] = ok[\"mode_val\"]\n",
    "    else:\n",
    "        # Oran kontrolü: mode / non_null_count >= min_ratio\n",
    "        def _mode_ratio(sub):\n",
    "            s = sub.dropna()\n",
    "            if len(s) == 0:\n",
    "                return 0.0\n",
    "            m = safe_mode(s)\n",
    "            return (s == m).mean() if m is not None else 0.0\n",
    "\n",
    "        stats[\"mode_ratio\"] = grp.apply(lambda s: _mode_ratio(s)).values\n",
    "        ok = stats.query(\"non_null_count >= @min_count and mode_ratio >= @min_ratio\").copy()\n",
    "        ok[\"use_val\"] = ok[\"mode_val\"]\n",
    "\n",
    "    # Hızlı join için anahtarı üret\n",
    "    key_df = df[group_cols].copy()\n",
    "    key_df[\"_row_id\"] = np.arange(len(df))\n",
    "    ok_key = ok[group_cols + [\"use_val\"]].copy()\n",
    "\n",
    "    # Sadece eksik satırlar\n",
    "    to_fill = df[target_col].isna()\n",
    "    merged = key_df.loc[to_fill].merge(ok_key, on=group_cols, how=\"left\")\n",
    "    # Eşleşenlerde doldur\n",
    "    hit = merged[\"use_val\"].notna()\n",
    "    df.loc[merged.loc[hit, \"_row_id\"], target_col] = merged.loc[hit, \"use_val\"].values\n",
    "\n",
    "    filled = before_missing - df[target_col].isna().sum()\n",
    "    return int(filled)\n",
    "\n",
    "# ---- Başlangıç istatistikleri ----\n",
    "def null_counts(d):\n",
    "    return d.isna().sum()[[\"kan_grubu\",\"bolum\",\"tanilar\",\"uygulama_yerleri\"]]\n",
    "\n",
    "print(\"\\nBAŞLANGIÇ NaN SAYILARI:\")\n",
    "print(null_counts(df))\n",
    "\n",
    "# ============================================\n",
    "# 1) Hasta bazlı doldurma (en güvenilir)\n",
    "# ============================================\n",
    "filled_patient = {\"bolum\":0, \"tanilar\":0, \"uygulama_yerleri\":0}\n",
    "for col in [\"bolum\", \"tanilar\", \"uygulama_yerleri\"]:\n",
    "    filled_patient[col] = fill_from_patient_consensus(df, col, id_col=\"hasta_no\")\n",
    "\n",
    "print(\"\\n[Hasta bazlı doldurulan]  \", filled_patient)\n",
    "\n",
    "# ============================================\n",
    "# 2) Grup bazlı doldurma (güvenlik eşikleriyle)\n",
    "# ============================================\n",
    "filled_group = {\"bolum\":0, \"tanilar\":0, \"uygulama_yerleri\":0}\n",
    "\n",
    "# 2.a) BÖLÜM: tedavi_adi + tedavi_suresi -> mode, min_count>=3, ratio>=0.7\n",
    "filled_group[\"bolum\"] = fill_from_group_mode(\n",
    "    df, \"bolum\",\n",
    "    group_cols=[\"tedavi_adi\",\"tedavi_suresi\"],\n",
    "    min_count=3, min_ratio=0.7, unanimous=False\n",
    ")\n",
    "\n",
    "# 2.b) UYGULAMA_YERLERİ: tedavi_adi + tedavi_suresi -> mode, min_count>=3, ratio>=0.7\n",
    "filled_group[\"uygulama_yerleri\"] = fill_from_group_mode(\n",
    "    df, \"uygulama_yerleri\",\n",
    "    group_cols=[\"tedavi_adi\",\"tedavi_suresi\"],\n",
    "    min_count=3, min_ratio=0.7, unanimous=False\n",
    ")\n",
    "\n",
    "# 2.c) TANI: Çok dikkatli — sadece TAM MUTABAKAT ile doldur.\n",
    "#          (tedavi_adi + tedavi_suresi + uygulama_yerleri) içinde nunique==1 ve count>=5\n",
    "filled_group[\"tanilar\"] = fill_from_group_mode(\n",
    "    df, \"tanilar\",\n",
    "    group_cols=[\"tedavi_adi\",\"tedavi_suresi\",\"uygulama_yerleri\"],\n",
    "    min_count=5, min_ratio=1.0, unanimous=True\n",
    ")\n",
    "\n",
    "print(\"\\n[Grup bazlı doldurulan]   \", filled_group)\n",
    "\n",
    "# ============================================\n",
    "# 3) Kalan eksikler için güvenli yedek: 'bilinmiyor'\n",
    "# ============================================\n",
    "# Kan grubu için tahmin yok -> direkt 'bilinmiyor'\n",
    "df[\"kan_grubu\"] = df[\"kan_grubu\"].apply(lambda x: \"bilinmiyor\" if is_na_like(x) else x)\n",
    "\n",
    "# Diğer hedef kolonlarda kalan NaN -> 'bilinmiyor'\n",
    "for col in [\"bolum\",\"tanilar\",\"uygulama_yerleri\"]:\n",
    "    df[col] = df[col].apply(lambda x: \"bilinmiyor\" if is_na_like(x) else x)\n",
    "\n",
    "print(\"\\nSON NaN SAYILARI (hepsi 0 olmalı):\")\n",
    "print(df[[\"kan_grubu\",\"bolum\",\"tanilar\",\"uygulama_yerleri\"]].isna().sum())\n",
    "\n",
    "# ============================================\n",
    "# Raporlama\n",
    "# ============================================\n",
    "print(\"\\n=== ÖZET ===\")\n",
    "print(\"Hasta bazlı doldurulanlar:\", filled_patient)\n",
    "print(\"Grup bazlı doldurulanlar :\", filled_group)\n",
    "\n",
    "# Örnek dağılımlar (bakış)\n",
    "for col in [\"bolum\",\"uygulama_yerleri\"]:\n",
    "    vc = df[col].value_counts().head(10)\n",
    "    print(f\"\\nTop {col} değerleri:\")\n",
    "    print(vc)\n",
    "\n",
    "# ============================================\n",
    "# Kayıt\n",
    "# ============================================\n",
    "out_csv  = \"/kaggle/working/dataset_imputed.csv\"\n",
    "out_xlsx = \"/kaggle/working/dataset_imputed.xlsx\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "try:\n",
    "    df.to_excel(out_xlsx, index=False)\n",
    "except Exception as e:\n",
    "    print(\"Excel yazım uyarısı:\", e)\n",
    "\n",
    "print(\"\\nKaydedildi:\")\n",
    "print(out_csv)\n",
    "print(out_xlsx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f3320ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T20:06:44.385026Z",
     "iopub.status.busy": "2025-09-05T20:06:44.384459Z",
     "iopub.status.idle": "2025-09-05T20:06:45.630226Z",
     "shell.execute_reply": "2025-09-05T20:06:45.628943Z"
    },
    "papermill": {
     "duration": 1.255244,
     "end_time": "2025-09-05T20:06:45.632286",
     "exception": false,
     "start_time": "2025-09-05T20:06:44.377042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKUNAN: /kaggle/working/Clean_Data_Case_DT_2025_clean_final.xlsx\n",
      "Şekil: (511, 13)\n",
      "\n",
      "[Cinsiyet Dağılımı]\n",
      "cinsiyet\n",
      "Kadın    304\n",
      "Erkek    207\n",
      "Name: count, dtype: int64\n",
      "\n",
      "BAŞLANGIÇ NaN SAYILARI (hedef kolonlar):\n",
      "kan_grubu           136\n",
      "bolum                 2\n",
      "tanilar               0\n",
      "uygulama_yerleri     34\n",
      "dtype: int64\n",
      "\n",
      "[Hasta bazlı doldurulan]  {'bolum': 0, 'tanilar': 0, 'uygulama_yerleri': 11}\n",
      "\n",
      "[Grup bazlı doldurulan]   {'bolum': 0, 'tanilar': 0, 'uygulama_yerleri': 0}\n",
      "\n",
      "SON NaN SAYILARI (hepsi 0 olmalı):\n",
      "kan_grubu           0\n",
      "bolum               0\n",
      "tanilar             0\n",
      "uygulama_yerleri    0\n",
      "dtype: int64\n",
      "\n",
      "Top bolum değerleri:\n",
      "bolum\n",
      "Fiziksel Tıp Ve Rehabilitasyon                    313\n",
      "Fiziksel Tıp Ve Rehabilitasyon,Solunum Merkezi    128\n",
      "Ortopedi Ve Travmatoloji                           40\n",
      "İç Hastalıkları                                     9\n",
      "Nöroloji                                            5\n",
      "Kardiyoloji                                         4\n",
      "Genel Cerrahi                                       3\n",
      "bilinmiyor                                          2\n",
      "Tıbbi Onkoloji                                      2\n",
      "Göğüs Hastalıkları                                  2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top uygulama_yerleri değerleri:\n",
      "uygulama_yerleri\n",
      "bel                        104\n",
      "boyun                       69\n",
      "tum vucut bolgesi           47\n",
      "diz                         37\n",
      "sol omuz bolgesi            31\n",
      "bilinmiyor                  23\n",
      "sag omuz bolgesi            23\n",
      "sol el bilek bolgesi        21\n",
      "sag ayak bilegi bolgesi     19\n",
      "sirt                        17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Kaydedildi:\n",
      "/kaggle/working/dataset_final_imputed.csv\n",
      "/kaggle/working/dataset_final_imputed.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Tek Pipeline: Cinsiyet Doldurma + Güvenli İmputasyon\n",
    "# (+ Alerji/Kronik/Tanı Normalizasyonları)\n",
    "# (Hasta -> Grup -> 'bilinmiyor')\n",
    "# ============================================================\n",
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Girdi yolları (ilk bulunanı okur) ----\n",
    "PREFERRED = [\n",
    "    \"/kaggle/working/Clean_Data_Case_DT_2025_clean_final.xlsx\",\n",
    "]\n",
    "\n",
    "def _read_any(paths):\n",
    "    for p in paths:\n",
    "        try:\n",
    "            if p.lower().endswith(\".xlsx\"):\n",
    "                return pd.read_excel(p), p\n",
    "            elif p.lower().endswith(\".csv\"):\n",
    "                return pd.read_csv(p), p\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise FileNotFoundError(\"Okunabilir bir dosya bulunamadı; PREFERRED listesini güncelleyin.\")\n",
    "\n",
    "df, in_path = _read_any(PREFERRED)\n",
    "print(\"OKUNAN:\", in_path)\n",
    "print(\"Şekil:\", df.shape)\n",
    "\n",
    "# ---- Beklenen kolonlar ----\n",
    "REQUIRED = [\n",
    "    \"hasta_no\",\"yas\",\"cinsiyet\",\"kan_grubu\",\"uyruk\",\"kronik_hastalik\",\n",
    "    \"bolum\",\"alerji\",\"tanilar\",\"tedavi_adi\",\"tedavi_suresi\",\n",
    "    \"uygulama_yerleri\",\"uygulama_suresi\"\n",
    "]\n",
    "missing_cols = [c for c in REQUIRED if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Eksik kolon(lar): {missing_cols}\")\n",
    "\n",
    "# ============================================================\n",
    "# 0) Yardımcılar\n",
    "# ============================================================\n",
    "def is_na_like(x):\n",
    "    if x is None:\n",
    "        return True\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"nan\",\"none\",\"null\"}:\n",
    "        return True\n",
    "    return pd.isna(x)\n",
    "\n",
    "def safe_mode(series: pd.Series):\n",
    "    s = series.dropna()\n",
    "    if s.empty:\n",
    "        return None\n",
    "    m = s.mode(dropna=True)\n",
    "    return m.iloc[0] if not m.empty else None\n",
    "\n",
    "def unique_if_consistent(series: pd.Series):\n",
    "    s = series.dropna().unique()\n",
    "    return s[0] if len(s) == 1 else None\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    if is_na_like(s): \n",
    "        return \"\"\n",
    "    s = str(s).casefold()\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# ============================================================\n",
    "# 1) Alerji & Kronik Hastalık NaN -> \"yok\"\n",
    "# ============================================================\n",
    "for col in [\"alerji\", \"kronik_hastalik\"]:\n",
    "    df[col] = df[col].apply(lambda x: \"yok\" if is_na_like(x) else str(x).strip())\n",
    "\n",
    "# ============================================================\n",
    "# 1.5) NORMALİZASYON EKLEMELERİ (senin istediklerin)\n",
    "#      - alerji: volteren->voltaren, yer fistigi->yer fıstığı\n",
    "#      - kronik: hiportiroidizm->hipotiroidizm\n",
    "#      - tanılar: bel/sırt dönüşümleri (ters yön)\n",
    "# ============================================================\n",
    "import unicodedata\n",
    "\n",
    "def _split_multi_cell(text):\n",
    "    # Çoklu değerleri parçala (',' veya ';')\n",
    "    parts = re.split(r\"[;,]\", str(text))\n",
    "    # trim + lower (casefold) + boşları at\n",
    "    return [p.strip().casefold() for p in parts if p and p.strip()]\n",
    "\n",
    "def _join_multi_cell(items):\n",
    "    # Tekrarları at, stabil sıralama için set yerine liste filtrele\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for it in items:\n",
    "        if it not in seen:\n",
    "            seen.add(it)\n",
    "            out.append(it)\n",
    "    return \", \".join(out)\n",
    "\n",
    "# --- ALERJİ normalizasyonu ---\n",
    "_alerji_map = {\n",
    "    \"volteren\": \"voltaren\",\n",
    "    \"voltaren\": \"voltaren\",\n",
    "    \"yer fistigi\": \"yer fıstığı\",\n",
    "    \"yer fıstığı\": \"yer fıstığı\",\n",
    "    \"yok\": \"yok\",\n",
    "}\n",
    "\n",
    "def normalize_alerji_cell(x):\n",
    "    if is_na_like(x): \n",
    "        return \"yok\"\n",
    "    toks = _split_multi_cell(x)\n",
    "    norm = [_alerji_map.get(tok, tok) for tok in toks]\n",
    "    return _join_multi_cell(norm)\n",
    "\n",
    "df[\"alerji\"] = df[\"alerji\"].apply(normalize_alerji_cell)\n",
    "\n",
    "# --- KRONİK normalizasyonu ---\n",
    "_kronik_map = {\n",
    "    \"hiportiroidizm\": \"hipotiroidizm\",\n",
    "    # istersen buraya başka düzeltmeler ekleyebilirsin\n",
    "}\n",
    "\n",
    "def normalize_kronik_cell(x):\n",
    "    if is_na_like(x): \n",
    "        return \"yok\"\n",
    "    toks = _split_multi_cell(x)\n",
    "    norm = [_kronik_map.get(tok, tok) for tok in toks]\n",
    "    return _join_multi_cell(norm)\n",
    "\n",
    "df[\"kronik_hastalik\"] = df[\"kronik_hastalik\"].apply(normalize_kronik_cell)\n",
    "\n",
    "# --- TANILAR normalizasyonu (ters yön) ---\n",
    "def normalize_tani_text_reverse(s: str) -> str:\n",
    "    if is_na_like(s):\n",
    "        return \"\"\n",
    "    t = str(s).casefold()\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # Önce spesifik kalıplar: bel ağrısı → lomber ağrısı\n",
    "    t = re.sub(r\"\\bbel ağrısı\\b\", \"lomber ağrısı\", t)\n",
    "    t = re.sub(r\"\\bbel agrisi\\b\", \"lomber ağrısı\", t)  # ASCII varyant\n",
    "\n",
    "    # Sonra tek kelime karşılıkları (kelime sınırıyla)\n",
    "    t = re.sub(r\"\\bbel\\b\", \"lomber\", t)\n",
    "\n",
    "    # Sırt → dorsalji\n",
    "    t = re.sub(r\"\\bsırt ağrısı\\b\", \"dorsalji\", t)\n",
    "    t = re.sub(r\"\\bsirt agrisi\\b\", \"dorsalji\", t)      # ASCII varyant\n",
    "    t = re.sub(r\"\\bsırt\\b\", \"dorsalji\", t)\n",
    "    t = re.sub(r\"\\bsirt\\b\", \"dorsalji\", t)\n",
    "\n",
    "    return t\n",
    "\n",
    "df[\"tanilar\"] = df[\"tanilar\"].astype(str).apply(normalize_tani_text_reverse)\n",
    "\n",
    "# ============================================================\n",
    "# 2) Cinsiyet Doldurma (Kural + Oransal)\n",
    "# ============================================================\n",
    "COL_CINS, COL_KRON, COL_TANI, COL_ALER = \"cinsiyet\", \"kronik_hastalik\", \"tanilar\", \"alerji\"\n",
    "\n",
    "# Kural kümeleri (genişletilebilir)\n",
    "female_patterns = [\n",
    "    r\"\\b(rahim|uterus|serviks|servical|servicalis)\\b\",\n",
    "    r\"\\b(over|ovary|ovaryan|ovarian|over kisti|polikistik over|pko)\\b\",\n",
    "    r\"\\b(meme|endometrioz|endometriozis)\\b\",\n",
    "    r\"\\b(gebelik|hamile|gravid|lohusalık|menopoz|adet|dismenore)\\b\",\n",
    "    r\"\\b(tiroid|hipotiroid|hipertiroid|tiroidit)\\b\",\n",
    "    r\"\\b(myasteni|myasthenia)\\b\",\n",
    "    r\"\\b(polimiyozit|polimiyozit)\\b\",\n",
    "]\n",
    "male_patterns = [\n",
    "    r\"\\b(prostat|bph|benign prostat hiperplazisi)\\b\",\n",
    "    r\"\\b(testis|testik|penil|epididim|varikosel|erkek infertil)\\b\",\n",
    "    r\"\\b(hipospad|hypospad)\\b\",\n",
    "    r\"\\b(duchenne|becker)\\b\",\n",
    "]\n",
    "female_re = re.compile(\"|\".join(female_patterns))\n",
    "male_re   = re.compile(\"|\".join(male_patterns))\n",
    "\n",
    "def _std_gender(x):\n",
    "    if is_na_like(x): \n",
    "        return np.nan\n",
    "    s = str(x).strip().casefold()\n",
    "    if s in {\"kadin\", \"kadın\", \"female\", \"f\", \"k\", \"woman\"}:\n",
    "        return \"Kadın\"\n",
    "    if s in {\"erkek\", \"male\", \"m\", \"e\", \"man\"}:\n",
    "        return \"Erkek\"\n",
    "    return np.nan\n",
    "\n",
    "df[COL_CINS] = df[COL_CINS].apply(_std_gender)\n",
    "\n",
    "mask_missing = df[COL_CINS].isna()\n",
    "comb_text = (df[COL_KRON].fillna(\"\").astype(str) + \" \" + df[COL_TANI].fillna(\"\").astype(str)).apply(_norm)\n",
    "\n",
    "def infer_gender_from_text(txt: str):\n",
    "    if not txt:\n",
    "        return np.nan\n",
    "    has_f = bool(female_re.search(txt))\n",
    "    has_m = bool(male_re.search(txt))\n",
    "    if has_f and not has_m:\n",
    "        return \"Kadın\"\n",
    "    if has_m and not has_f:\n",
    "        return \"Erkek\"\n",
    "    return np.nan\n",
    "\n",
    "df.loc[mask_missing, COL_CINS] = comb_text[mask_missing].apply(infer_gender_from_text)\n",
    "\n",
    "mask_missing = df[COL_CINS].isna()\n",
    "if mask_missing.any():\n",
    "    vc = df[COL_CINS].value_counts(dropna=True)\n",
    "    n_known = vc.sum()\n",
    "    if n_known == 0:\n",
    "        p_female = p_male = 0.5\n",
    "    else:\n",
    "        p_female = float(vc.get(\"Kadın\", 0)) / n_known\n",
    "        p_male   = float(vc.get(\"Erkek\", 0)) / n_known\n",
    "        eps = 1e-9\n",
    "        p_female = max(p_female, eps)\n",
    "        p_male   = max(p_male, eps)\n",
    "        s = p_female + p_male\n",
    "        p_female, p_male = p_female/s, p_male/s\n",
    "    rng = np.random.default_rng(42)\n",
    "    fill_vals = rng.choice([\"Kadın\", \"Erkek\"], size=mask_missing.sum(), p=[p_female, p_male])\n",
    "    df.loc[mask_missing, COL_CINS] = fill_vals\n",
    "\n",
    "print(\"\\n[Cinsiyet Dağılımı]\")\n",
    "print(df[COL_CINS].value_counts())\n",
    "\n",
    "# ============================================================\n",
    "# 3) Güvenli Doldurma (Hasta -> Grup -> 'bilinmiyor')\n",
    "#    Hedef kolonlar: kan_grubu, bolum, tanilar, uygulama_yerleri\n",
    "# ============================================================\n",
    "def fill_from_patient_consensus(df, target_col, id_col=\"hasta_no\"):\n",
    "    before_missing = df[target_col].isna().sum()\n",
    "    cons_map = (\n",
    "        df.groupby(id_col)[target_col]\n",
    "          .apply(unique_if_consistent)\n",
    "          .dropna()\n",
    "          .to_dict()\n",
    "    )\n",
    "    mask = df[target_col].isna() & df[id_col].isin(cons_map.keys())\n",
    "    df.loc[mask, target_col] = df.loc[mask, id_col].map(cons_map)\n",
    "    return int(before_missing - df[target_col].isna().sum())\n",
    "\n",
    "def fill_from_group_mode(df, target_col, group_cols, min_count=3, min_ratio=0.7, unanimous=False):\n",
    "    before_missing = df[target_col].isna().sum()\n",
    "    grp = df.groupby(group_cols)[target_col]\n",
    "    stats = grp.agg(\n",
    "        non_null_count=lambda s: s.notna().sum(),\n",
    "        nunique=lambda s: s.dropna().nunique(),\n",
    "        mode_val=lambda s: safe_mode(s),\n",
    "    ).reset_index()\n",
    "\n",
    "    if unanimous:\n",
    "        ok = stats.query(\"non_null_count >= @min_count and nunique == 1\").copy()\n",
    "        ok[\"use_val\"] = ok[\"mode_val\"]\n",
    "    else:\n",
    "        def _mode_ratio(sub):\n",
    "            s = sub.dropna()\n",
    "            if len(s) == 0: return 0.0\n",
    "            m = safe_mode(s)\n",
    "            return (s == m).mean() if m is not None else 0.0\n",
    "        stats[\"mode_ratio\"] = grp.apply(lambda s: _mode_ratio(s)).values\n",
    "        ok = stats.query(\"non_null_count >= @min_count and mode_ratio >= @min_ratio\").copy()\n",
    "        ok[\"use_val\"] = ok[\"mode_val\"]\n",
    "\n",
    "    key_df = df[group_cols].copy()\n",
    "    key_df[\"_row_id\"] = np.arange(len(df))\n",
    "    ok_key = ok[group_cols + [\"use_val\"]].copy()\n",
    "\n",
    "    to_fill = df[target_col].isna()\n",
    "    merged = key_df.loc[to_fill].merge(ok_key, on=group_cols, how=\"left\")\n",
    "    hit = merged[\"use_val\"].notna()\n",
    "    df.loc[merged.loc[hit, \"_row_id\"], target_col] = merged.loc[hit, \"use_val\"].values\n",
    "\n",
    "    return int(before_missing - df[target_col].isna().sum())\n",
    "\n",
    "def null_counts(d):\n",
    "    return d.isna().sum()[[\"kan_grubu\",\"bolum\",\"tanilar\",\"uygulama_yerleri\"]]\n",
    "\n",
    "print(\"\\nBAŞLANGIÇ NaN SAYILARI (hedef kolonlar):\")\n",
    "print(null_counts(df))\n",
    "\n",
    "# 3.1) Hasta bazlı\n",
    "filled_patient = {\"bolum\":0, \"tanilar\":0, \"uygulama_yerleri\":0}\n",
    "for col in [\"bolum\", \"tanilar\", \"uygulama_yerleri\"]:\n",
    "    filled_patient[col] = fill_from_patient_consensus(df, col, id_col=\"hasta_no\")\n",
    "print(\"\\n[Hasta bazlı doldurulan] \", filled_patient)\n",
    "\n",
    "# 3.2) Grup bazlı\n",
    "filled_group = {\"bolum\":0, \"tanilar\":0, \"uygulama_yerleri\":0}\n",
    "filled_group[\"bolum\"] = fill_from_group_mode(\n",
    "    df, \"bolum\",\n",
    "    group_cols=[\"tedavi_adi\",\"tedavi_suresi\"],\n",
    "    min_count=3, min_ratio=0.7, unanimous=False\n",
    ")\n",
    "filled_group[\"uygulama_yerleri\"] = fill_from_group_mode(\n",
    "    df, \"uygulama_yerleri\",\n",
    "    group_cols=[\"tedavi_adi\",\"tedavi_suresi\"],\n",
    "    min_count=3, min_ratio=0.7, unanimous=False\n",
    ")\n",
    "# Tanılar için sadece TAM MUTABAKAT ve daha sıkı eşik\n",
    "filled_group[\"tanilar\"] = fill_from_group_mode(\n",
    "    df, \"tanilar\",\n",
    "    group_cols=[\"tedavi_adi\",\"tedavi_suresi\",\"uygulama_yerleri\"],\n",
    "    min_count=5, min_ratio=1.0, unanimous=True\n",
    ")\n",
    "print(\"\\n[Grup bazlı doldurulan]  \", filled_group)\n",
    "\n",
    "# 3.3) Fallback: 'bilinmiyor'\n",
    "df[\"kan_grubu\"] = df[\"kan_grubu\"].apply(lambda x: \"bilinmiyor\" if is_na_like(x) else x)\n",
    "for col in [\"bolum\",\"tanilar\",\"uygulama_yerleri\"]:\n",
    "    df[col] = df[col].apply(lambda x: \"bilinmiyor\" if is_na_like(x) else x)\n",
    "\n",
    "print(\"\\nSON NaN SAYILARI (hepsi 0 olmalı):\")\n",
    "print(df[[\"kan_grubu\",\"bolum\",\"tanilar\",\"uygulama_yerleri\"]].isna().sum())\n",
    "\n",
    "# Kısa dağılım bakışı\n",
    "for col in [\"bolum\",\"uygulama_yerleri\"]:\n",
    "    print(f\"\\nTop {col} değerleri:\")\n",
    "    print(df[col].value_counts().head(10))\n",
    "\n",
    "# ============================================================\n",
    "# Kayıt\n",
    "# ============================================================\n",
    "out_csv  = \"/kaggle/working/dataset_final_imputed.csv\"\n",
    "out_xlsx = \"/kaggle/working/dataset_final_imputed.xlsx\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "try:\n",
    "    df.to_excel(out_xlsx, index=False)\n",
    "except Exception as e:\n",
    "    print(\"Excel yazım uyarısı:\", e)\n",
    "\n",
    "print(\"\\nKaydedildi:\")\n",
    "print(out_csv)\n",
    "print(out_xlsx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e2d6561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T20:06:45.646759Z",
     "iopub.status.busy": "2025-09-05T20:06:45.646412Z",
     "iopub.status.idle": "2025-09-05T20:07:06.956035Z",
     "shell.execute_reply": "2025-09-05T20:07:06.954398Z"
    },
    "papermill": {
     "duration": 21.319408,
     "end_time": "2025-09-05T20:07:06.957860",
     "exception": false,
     "start_time": "2025-09-05T20:06:45.638452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKUNAN: /kaggle/working/dataset_final_imputed.xlsx | Şekil: (511, 13)\n",
      "TOP_K=20 | CV MAE=1.770 ± 0.110 | n_features=119\n",
      "TOP_K=25 | CV MAE=1.731 ± 0.115 | n_features=129\n",
      "TOP_K=30 | CV MAE=1.745 ± 0.108 | n_features=139\n",
      "\n",
      ">>> EN İYİ TOP_K: 25 | MAE=1.731 ± 0.115 | n_features=129\n",
      "\n",
      "Kaydedildi (TEK ÇIKTI):\n",
      "/kaggle/working/dataset_model_ready_best_top25.csv\n",
      "/kaggle/working/dataset_model_ready_best_top25.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# TOP-K (20/25/30) KARŞILAŞTIR -> EN İYİ K'YI SEÇ -> TEK ÇIKTI\n",
    "# Kaggle uyumlu (orijinal metin sütunları tamamen kaldırılır)\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# ------------------ Parametreler ------------------\n",
    "IN_PATH = \"/kaggle/working/dataset_final_imputed.xlsx\"\n",
    "TOP_K_LIST = [20, 25, 30]     # denenecek K değerleri\n",
    "N_SPLITS = 5                  # 5-fold CV\n",
    "RANDOM_STATE = 42\n",
    "SCORING = \"neg_mean_absolute_error\"  # MAE\n",
    "\n",
    "# ------------------ Yardımcı Fonksiyonlar ------------------\n",
    "def split_multi(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    parts = re.split(r\"[;,]\", str(x))\n",
    "    return [p.strip().casefold() for p in parts if p.strip()]\n",
    "\n",
    "def ohe_multilabel(series):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    lists = series.apply(split_multi)\n",
    "    arr = mlb.fit_transform(lists)\n",
    "    cols = [f\"{series.name}__{c}\" for c in mlb.classes_]\n",
    "    return pd.DataFrame(arr, index=series.index, columns=cols).astype(\"int8\")\n",
    "\n",
    "def topk_multilabel(series, top_list):\n",
    "    lists = series.apply(split_multi)\n",
    "    top_set = set(top_list)\n",
    "    data = {}\n",
    "    for lab in top_list:\n",
    "        data[f\"{series.name}__{lab}\"] = lists.apply(lambda L: int(lab in set(L))).astype(\"int8\")\n",
    "    data[f\"{series.name}__other\"] = lists.apply(lambda L: int(any(l for l in L if l not in top_set))).astype(\"int8\")\n",
    "    return pd.DataFrame(data, index=series.index)\n",
    "\n",
    "def onehot_drop_first(df, col, prefix=None):\n",
    "    return pd.get_dummies(\n",
    "        df[col].astype(str).str.strip().str.lower(),\n",
    "        prefix=(prefix or col),\n",
    "        drop_first=True,\n",
    "        dtype=\"int8\"\n",
    "    )\n",
    "\n",
    "def build_dataset(df, TOP_K):\n",
    "    \"\"\"Verilen TOP_K için tamamen sayısal ve encode edilmiş dataset döndürür.\"\"\"\n",
    "    # Sayısallar\n",
    "    numeric_cols = [\"yas\", \"tedavi_suresi\", \"uygulama_suresi\"]\n",
    "\n",
    "    # Cinsiyet (binary)\n",
    "    cinsiyet_enc = (\n",
    "        df[\"cinsiyet\"].astype(str).str.casefold().str.strip().map({\"kadın\":1, \"erkek\":0})\n",
    "        .astype(\"Int8\").fillna(0)\n",
    "    ).to_frame(\"cinsiyet_enc\")\n",
    "\n",
    "    # One-hot (drop_first)\n",
    "    kan_ohe   = onehot_drop_first(df, \"kan_grubu\", prefix=\"kan\")\n",
    "    uyruk_ohe = onehot_drop_first(df, \"uyruk\",      prefix=\"uyruk\")\n",
    "    bolum_ohe = onehot_drop_first(df, \"bolum\",      prefix=\"bolum\")\n",
    "\n",
    "    # Multi-label tam OHE\n",
    "    uyg_ohe  = ohe_multilabel(df[\"uygulama_yerleri\"])\n",
    "    alr_ohe  = ohe_multilabel(df[\"alerji\"])\n",
    "    kron_ohe = ohe_multilabel(df[\"kronik_hastalik\"])\n",
    "\n",
    "    # Tanı frekans → Top-K + other\n",
    "    tani_lists = df[\"tanilar\"].apply(split_multi)\n",
    "    cnt_tani = Counter(); [cnt_tani.update(lst) for lst in tani_lists]\n",
    "    tani_top_list = [lab for lab, _ in cnt_tani.most_common(TOP_K)]\n",
    "    tani_top_ohe = topk_multilabel(df[\"tanilar\"], tani_top_list)\n",
    "\n",
    "    # Tedavi adı frekans → Top-K + other\n",
    "    tedavi_clean = df[\"tedavi_adi\"].astype(str).str.strip().str.casefold()\n",
    "    tedavi_top_list = list(tedavi_clean.value_counts().head(TOP_K).index)\n",
    "    tedavi_top_or_other = tedavi_clean.where(tedavi_clean.isin(tedavi_top_list), other=\"other\")\n",
    "    tedavi_ohe = pd.get_dummies(tedavi_top_or_other, prefix=\"tedavi\", drop_first=False, dtype=\"int8\")\n",
    "\n",
    "    # Tümünü birleştir (yalnızca sayısal + encode edilmişler)\n",
    "    parts = [\n",
    "        df[numeric_cols], \n",
    "        cinsiyet_enc,\n",
    "        kan_ohe, uyruk_ohe, bolum_ohe,\n",
    "        uyg_ohe, alr_ohe, kron_ohe,\n",
    "        tani_top_ohe, tedavi_ohe\n",
    "    ]\n",
    "    Xy = pd.concat(parts, axis=1)\n",
    "    # Hedef değişkeni ayır\n",
    "    y = Xy[\"tedavi_suresi\"].astype(float).values\n",
    "    X = Xy.drop(columns=[\"tedavi_suresi\"])\n",
    "    # NaN güvenliği (kalırsa): 0 doldur\n",
    "    X = X.fillna(0)\n",
    "    return X, y, Xy.columns.tolist()\n",
    "\n",
    "# ------------------ Veriyi Yükle ------------------\n",
    "df = pd.read_excel(IN_PATH)\n",
    "print(f\"OKUNAN: {IN_PATH} | Şekil: {df.shape}\")\n",
    "\n",
    "# ------------------ Model & CV Hazırlığı ------------------\n",
    "cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "results = []   # (TOP_K, mean_MAE, std_MAE, n_features)\n",
    "\n",
    "# ------------------ Denemeler: 20/25/30 ------------------\n",
    "for K in TOP_K_LIST:\n",
    "    X, y, cols = build_dataset(df, K)\n",
    "    scores = cross_val_score(model, X, y, scoring=SCORING, cv=cv, n_jobs=-1)\n",
    "    mae_mean = -scores.mean()\n",
    "    mae_std  =  scores.std()\n",
    "    results.append((K, mae_mean, mae_std, X.shape[1]))\n",
    "    print(f\"TOP_K={K:>2} | CV MAE={mae_mean:.3f} ± {mae_std:.3f} | n_features={X.shape[1]}\")\n",
    "\n",
    "# ------------------ En iyi TOP_K'yı seç ------------------\n",
    "best_K, best_mae, best_std, best_feats = sorted(results, key=lambda t: t[1])[0]\n",
    "print(f\"\\n>>> EN İYİ TOP_K: {best_K} | MAE={best_mae:.3f} ± {best_std:.3f} | n_features={best_feats}\")\n",
    "\n",
    "# ------------------ En iyi K ile TEK FİNAL veri seti üret ------------------\n",
    "X_best, y_best, cols_best = build_dataset(df, best_K)\n",
    "final_df = pd.DataFrame(X_best, columns=[c for c in cols_best if c != \"tedavi_suresi\"])\n",
    "final_df[\"tedavi_suresi\"] = y_best  # hedef en sonda\n",
    "\n",
    "OUT_CSV  = f\"/kaggle/working/dataset_model_ready_best_top{best_K}.csv\"\n",
    "OUT_XLSX = f\"/kaggle/working/dataset_model_ready_best_top{best_K}.xlsx\"\n",
    "final_df.to_csv(OUT_CSV, index=False)\n",
    "try:\n",
    "    final_df.to_excel(OUT_XLSX, index=False)\n",
    "except Exception as e:\n",
    "    print(\"Excel yazım uyarısı:\", e)\n",
    "\n",
    "print(\"\\nKaydedildi (TEK ÇIKTI):\")\n",
    "print(OUT_CSV)\n",
    "print(OUT_XLSX)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8204862,
     "sourceId": 12964080,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 54.252791,
   "end_time": "2025-09-05T20:07:09.585463",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-05T20:06:15.332672",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
